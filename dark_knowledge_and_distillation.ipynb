{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dark_knowledge_and_distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0o3rRRZu0IT+fnz9WEmso",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandrumeterez/ai_notebooks/blob/master/dark_knowledge_and_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDUJMKsJZAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf8TulZTLaSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Teacher(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.fc1 = nn.Linear(784, 1800)\n",
        "        self.fc2 = nn.Linear(1800, 1800)\n",
        "        self.fc3 = nn.Linear(1800, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training is False:\n",
        "            self.T = 1\n",
        "        x = self.fc1(x)\n",
        "        x = F.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        # x = F.softmax(x / self.T, dim=0)\n",
        "        return x / self.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D49s3oGHM1ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Student(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.fc1 = nn.Linear(784, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training is False:\n",
        "            self.T = 1\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        # x = F.softmax(x / self.T, dim=0)\n",
        "        return x / self.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pOdi0QNS-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_ds = MNIST(root='.', train=True, download=True, transform=preprocess)\n",
        "test_ds = MNIST(root='.', train=False, download=True, transform=preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSjRLC1EODon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18v2QdLROl3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        inputs = inputs.view(BATCH_SIZE, -1)\n",
        "        predictions = model(inputs)\n",
        "        loss = criterion(predictions, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(predictions.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return epoch_loss / len(train_loader), correct / total\n",
        "\n",
        "def train_distill(model, teacher, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        inputs = inputs.view(BATCH_SIZE, -1)\n",
        "        predictions = model(inputs)\n",
        "        teacher_labels = teacher(inputs)\n",
        "\n",
        "        loss = criterion(predictions, teacher_labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(predictions.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return epoch_loss / len(train_loader), correct / total\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            inputs = inputs.view(BATCH_SIZE, -1)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(predictions.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return epoch_loss / len(test_loader), correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB9kdKg5PYv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "10eca27d-ca36-4031-c5fd-47df919d6751"
      },
      "source": [
        "TEMPERATURE = 20\n",
        "N_EPOCHS = 10\n",
        "\n",
        "\n",
        "# Train teacher\n",
        "teacher = Teacher(TEMPERATURE)\n",
        "teacher.to(DEVICE)\n",
        "\n",
        "optimizer = SGD(teacher.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    train_loss, train_acc = train(teacher, train_loader, criterion, optimizer)\n",
        "    print(f'\\tTrain loss: {train_loss:.4f}, Train acc: {train_acc * 100:.2f}%')\n",
        "    test_loss, test_acc = test(teacher, test_loader, criterion)\n",
        "    print(f'\\tTest loss: {test_loss:.4f}, Test acc: {test_acc * 100:.2f}%')\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\tTrain loss: 2.3023, Train acc: 11.50%\n",
            "\tTest loss: 2.2941, Test acc: 13.00%\n",
            "Epoch: 2\n",
            "\tTrain loss: 2.2114, Train acc: 37.69%\n",
            "\tTest loss: 2.1019, Test acc: 57.13%\n",
            "Epoch: 3\n",
            "\tTrain loss: 1.9041, Train acc: 63.98%\n",
            "\tTest loss: 1.6212, Test acc: 69.17%\n",
            "Epoch: 4\n",
            "\tTrain loss: 1.3241, Train acc: 72.31%\n",
            "\tTest loss: 1.0364, Test acc: 77.19%\n",
            "Epoch: 5\n",
            "\tTrain loss: 0.8993, Train acc: 77.96%\n",
            "\tTest loss: 0.7579, Test acc: 80.58%\n",
            "Epoch: 6\n",
            "\tTrain loss: 0.7065, Train acc: 80.94%\n",
            "\tTest loss: 0.6305, Test acc: 83.07%\n",
            "Epoch: 7\n",
            "\tTrain loss: 0.6087, Train acc: 83.10%\n",
            "\tTest loss: 0.5591, Test acc: 84.32%\n",
            "Epoch: 8\n",
            "\tTrain loss: 0.5485, Train acc: 84.60%\n",
            "\tTest loss: 0.5015, Test acc: 85.66%\n",
            "Epoch: 9\n",
            "\tTrain loss: 0.5068, Train acc: 85.48%\n",
            "\tTest loss: 0.4754, Test acc: 86.70%\n",
            "Epoch: 10\n",
            "\tTrain loss: 0.4772, Train acc: 86.30%\n",
            "\tTest loss: 0.4464, Test acc: 87.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYZbs8ASVh7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4f42bdce-0b33-4614-a091-f89f72c2f3e8"
      },
      "source": [
        "# Train student without distilation\n",
        "student = Student(TEMPERATURE)\n",
        "student.to(DEVICE)\n",
        "\n",
        "optimizer = SGD(student.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    train_loss, train_acc = train(student, train_loader, criterion, optimizer)\n",
        "    print(f'\\tTrain loss: {train_loss:.4f}, Train acc: {train_acc * 100:.2f}%')\n",
        "    test_loss, test_acc = test(student, test_loader, criterion)\n",
        "    print(f'\\tTest loss: {test_loss:.4f}, Test acc: {test_acc * 100:.2f}%')\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\tTrain loss: 2.3024, Train acc: 12.06%\n",
            "\tTest loss: 2.3247, Test acc: 11.96%\n",
            "Epoch: 2\n",
            "\tTrain loss: 2.3012, Train acc: 11.84%\n",
            "\tTest loss: 2.2771, Test acc: 13.90%\n",
            "Epoch: 3\n",
            "\tTrain loss: 2.2505, Train acc: 15.71%\n",
            "\tTest loss: 2.2144, Test acc: 17.09%\n",
            "Epoch: 4\n",
            "\tTrain loss: 2.1762, Train acc: 17.44%\n",
            "\tTest loss: 2.1163, Test acc: 18.59%\n",
            "Epoch: 5\n",
            "\tTrain loss: 2.0693, Train acc: 18.65%\n",
            "\tTest loss: 1.9909, Test acc: 19.36%\n",
            "Epoch: 6\n",
            "\tTrain loss: 1.9410, Train acc: 23.20%\n",
            "\tTest loss: 1.8519, Test acc: 27.70%\n",
            "Epoch: 7\n",
            "\tTrain loss: 1.8019, Train acc: 34.57%\n",
            "\tTest loss: 1.7072, Test acc: 42.78%\n",
            "Epoch: 8\n",
            "\tTrain loss: 1.6481, Train acc: 46.26%\n",
            "\tTest loss: 1.5370, Test acc: 51.26%\n",
            "Epoch: 9\n",
            "\tTrain loss: 1.4640, Train acc: 53.31%\n",
            "\tTest loss: 1.3334, Test acc: 58.48%\n",
            "Epoch: 10\n",
            "\tTrain loss: 1.2544, Train acc: 61.47%\n",
            "\tTest loss: 1.1167, Test acc: 67.92%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Qp_nCwZ9H6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "213126a0-822a-42e3-b73e-4b3d3055b409"
      },
      "source": [
        "# Train student with distilation\n",
        "student = Student(TEMPERATURE)\n",
        "student.to(DEVICE)\n",
        "\n",
        "optimizer = SGD(student.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "criterion_test = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    train_loss, train_acc = train_distill(student, teacher, train_loader, criterion, optimizer)\n",
        "    print(f'\\tTrain loss: {train_loss:.4f}, Train acc: {train_acc * 100:.2f}%')\n",
        "    test_loss, test_acc = test(student, test_loader, criterion_test)\n",
        "    print(f'\\tTest loss: {test_loss:.4f}, Test acc: {test_acc * 100:.2f}%')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\tTrain loss: 11.5638, Train acc: 9.86%\n",
            "\tTest loss: 2.3076, Test acc: 9.59%\n",
            "Epoch: 2\n",
            "\tTrain loss: 8.1813, Train acc: 23.03%\n",
            "\tTest loss: 1.4821, Test acc: 45.26%\n",
            "Epoch: 3\n",
            "\tTrain loss: 2.5671, Train acc: 61.33%\n",
            "\tTest loss: 0.8330, Test acc: 73.50%\n",
            "Epoch: 4\n",
            "\tTrain loss: 1.5226, Train acc: 75.77%\n",
            "\tTest loss: 0.7259, Test acc: 79.11%\n",
            "Epoch: 5\n",
            "\tTrain loss: 1.1257, Train acc: 80.68%\n",
            "\tTest loss: 0.6193, Test acc: 82.36%\n",
            "Epoch: 6\n",
            "\tTrain loss: 0.9079, Train acc: 82.43%\n",
            "\tTest loss: 0.5882, Test acc: 83.18%\n",
            "Epoch: 7\n",
            "\tTrain loss: 0.8350, Train acc: 83.28%\n",
            "\tTest loss: 0.5680, Test acc: 83.93%\n",
            "Epoch: 8\n",
            "\tTrain loss: 0.7895, Train acc: 83.94%\n",
            "\tTest loss: 0.5534, Test acc: 84.15%\n",
            "Epoch: 9\n",
            "\tTrain loss: 0.7507, Train acc: 84.45%\n",
            "\tTest loss: 0.5366, Test acc: 85.09%\n",
            "Epoch: 10\n",
            "\tTrain loss: 0.7245, Train acc: 84.85%\n",
            "\tTest loss: 0.5244, Test acc: 85.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AglclQgMdEKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}